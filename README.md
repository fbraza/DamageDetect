# EDP electronic boxes detection and classification using AI



## Outline

Use artificial intelligence approaches to be able to detect the EDP electronic boxes from photos and assess the presence of any alterations. Based on this the ultimate goal is to guide the company in the decision making process related to the continuous maintenance of the boxes. 

In this project we set two main goals: detect the EDP boxes and evaluate their states. In this regards we use a dataset generated by the company that contains **NUMBER OF TOTAL** photos. The photos depicted several situations (i.e., zoom on the corner, zoom inside, zoom on the lock, zoom on the boxes, far from the boxes). Consequently  and for the purpose of this proof-of-concept we decided to focus on photos where the full / part of the boxes were depicted. This leads us to use of 303 photos from the dataset. Discussion and perspectives will be proposed at the end of this report to both improve our approach and tackle the problems risen by the other 200 photos.

The global approach used here aims at fulfilling the following objectives:

- Detect the boxes in each photos using deep learning
- Cropping out the boxes and save the new pictures
- Used the cropped picture to feed a deep learning classification model that will determine the state of the EDP electronic box (**for Samuel**).

Figure 1 sums up this approach and highlight the main tools used in the process.

![](assets/Figure1.png)

*Figure 1: Overview of our workflow*

## Methodology & Results

#### Dataset

In this project we aim at the identification of 1 class of object: EDP electronic box. In total we got 303 images. Example images for each class are provided in Figure 2 below.

![](assets/Figure2.png)

*Figure 2: Some representative photos of our dataset*

The Computer Vision and Annotation Tool ([CVAT](https://github.com/openvinotoolkit/cvat)) has been used to label the images and export the bounding boxes data in YOLO format.

```bash
# Example of labeling data in YOLO format

Class Index     x          y          width      height
---------------------------------------------------------
0   			0.438271   0.523156   0.179000   0.191354

# Classes are 0 indexed
# x and y are the coordinates of the center point of the bounding box
# Values are normalized regarding size of the image
```

#### Data augmentation

We used traditional transformations that combined affine image transformations and color modifications. These transformations have been performed using the [Albumentations](https://github.com/albumentations-team/albumentations) python library. This library leverages `numpy`, `opencv` and `imgaug` python libraries through an easy to use API. The sequence of transformations can be seen below in the code snippet.

```python
A.Compose(
    [A.Resize(416, 416),
     A.Equalize(by_channels=True),
     A.RGBShiftr_shift_limit=(-30, 30), g_shift_limit=(-30, 30), b_shift_limit=(-30, 30), p=0.25),
     A.HorizontalFlip(p=0.35),
     A.VerticalFlip(p=0.35),
     A.ShiftScaleRotate(border_mode=cv2.BORDER_REPLICATE, p=0.35),
     A.RandomSnow(brightness_coeff=2.0, p=0.2)],
     A.BboxParams('yolo', ['class_labels'])
    )
```

Each image went through 75 distinct rounds of transformations which brings the total number of images to 22725. Our images have been spitted into training and validation sets at a `0.75|0.25` ratio. 

#### EDP boxes detection model training with YOLOv4

For EDP boxes detection we used the YOLOv4 architecture whom backbone network is based on the CSPDarknet53 ResNet. YOLO is a one-stage detector meaning that predictions for object localization and classification are done at the same time. Additionally and through its previous iterations the model significantly improves by adding Batch-norm, higher resolution, anchor boxes, objectness score to bounding box prediction and a detection in three granular step to improve the detection of smaller objects. From the user perspective YOLO proved to be very easy to use and setup. Indeed because of the time restriction when using the Google Colab free tier we decided to install locally all necessary drivers (NVIDIA, CUDA) and compile locally the [Darknet architecture](https://github.com/AlexeyAB/darknet). This has been done on a Linux computer running Ubuntu 20.04, with 32GB of RAM, NVIDIA GeForce GTX1060 graphic card with 6GB memory and an Intel i7 processor.

To evaluate the model we relied on two metrics: the **mean average precision** (mAP) and the **intersection over union** (IoU). The **average precision** (AP) is a way to get a fair idea of the model performance. It consists of computing the maximum precision we can get at different threshold of recall. Then we calculate the mean of these maximum precision. If we had several classes we need to get the AP for each class and then compute the mean again. This is why this metric is named **mean average precision**. In our case because we only have on class AP and mAP are equals. Object detection brings an additional complexity: what if the model detects the correct class but at the wrong location meaning that the bounding box is completely off. Surely this prediction should not be counted as positive. That is where the IoU comes handy and allows to determine whether the bounding box is located at the right location. Usually a threshold of 0.5 is set and results above are considered as good prediction. As such the corresponding mAP is noted as **mAP@0.5**. The principle of the IoU is depicted in Figure 3.

![](assets/Figure3.png)

*Figure 3: IoU Principle*

After training we assess the accuracy of our model on a validation set. Doing so revealed that our model yielded a mAP with a IoU threshold of 50% of 99.7%. 

```
# To get metrics on validation set run the following command
# ./darknet detector map data/obj.data cfg/yolov4-custom.cfg backup/yolov4-custom_5000.weights > ~/Documents/metrics.txt

calculation mAP (mean average precision)...
Detection layer: 139 - type = 28 
Detection layer: 150 - type = 28 
Detection layer: 161 - type = 28 

class_id = 0, name = box-closed, ap = 99.73%   	 (TP = 5669, FP = 81) 

for conf_thresh = 0.25, precision = 0.99, recall = 0.99, F1-score = 0.99 
for conf_thresh = 0.25, TP = 5669, FP = 81, FN = 52, average IoU = 87.53 % 

IoU threshold = 50 %, used Area-Under-Curve for each unique Recall 
mean average precision (mAP@0.50) = 0.997317, or 99.73 %
```

In practice running the predictions on raw images yielded 95% of efficacy to detect the EDP boxes (8 images yielded not accurate prediction and other had the good prediction but the bounding box was ex-centered). A representative set of predictions are shown in the figure below.

![](assets/Figure4.png)

*Figure 4: Representative results obtain from our object detection model*