# EDP electronic boxes detection and classification using AI



## Outline

Use artificial intelligence approaches to be able to detect the EDP electronic boxes from photos and assess the presence of any alterations. Based on this the ultimate goal is to guide the company in the decision making process related to the continuous maintenance of the boxes. 

In this project we set two main goals: detect the EDP boxes and evaluate their states. In this regards we use a dataset generated by the company that contains **to complete** photos. The photos depicted several situations (i.e., zoom on the corner, zoom inside, zoom on the lock, zoom on the boxes, far from the boxes). Consequently  and for the purpose of this proof-of-concept we decided to focus on photos where the full / part of the boxes were depicted. This leads us to use of 303 photos from the dataset. Discussion and perspectives will be proposed at the end of this report to both improve our approach and tackle the problems risen by the other 200 photos.

The global approach used here aims at fulfilling the following objectives:

- Detect the boxes in each photos using deep learning
- Cropping out the boxes and save the new pictures
- Used the cropped picture to feed a deep learning classification model that will determine the state of the EDP electronic box (**Samuel fill in**).

Figure 1 sums up this approach and highlight the main tools used in the process.



## Methodology & Results

#### Dataset

In this project we aim at the identification of 1 class of object: EDP electronic box. In total we got 303 images. Example images for each class are provided in Figure 2 below.



The Computer Vision and Annotation Tool ([CVAT](https://github.com/openvinotoolkit/cvat)) has been used to label the images and export the bounding boxes data in YOLO format.



#### Data augmentation

We used traditional transformations that combined affine image transformations and color modifications. These transformations have been performed using the [Albumentations](https://github.com/albumentations-team/albumentations) python library. This library leverages `numpy`, `opencv` and `imgaug` python libraries through an easy to use API. The sequence of transformations can be seen below in the code snippet.

```python
A.Compose(
    [A.Resize(416, 416),
     A.Equalize(by_channels=True),
     A.RGBShiftr_shift_limit=(-30, 30), g_shift_limit=(-30, 30), b_shift_limit=(-30, 30), p=0.25),
     A.HorizontalFlip(p=0.35),
     A.VerticalFlip(p=0.35),
     A.ShiftScaleRotate(border_mode=cv2.BORDER_REPLICATE, p=0.35),
     A.RandomSnow(brightness_coeff=2.0, p=0.2)],
     A.BboxParams('yolo', ['class_labels'])
    )
```

Each image went through 75 distinct rounds of transformations which brings the total number of images to 22725. Our images have been spitted into training and validation sets at a 0.75|0.25 ratio. 



#### EDP boxes detection model training with YOLOv4

For EDP boxes detection we used the YOLOv4 architecture whom backbone network is based on the CSPDarknet53 ResNet. YOLO is a one-stage detector meaning that predictions for object localization and classification are done at the same time. Additionally and through its previous iterations the model significantly improves by adding Batch-norm, higher resolution, anchor boxes, objectness score to bounding box prediction and a detection in three granular step to improve the detection of smaller objects. From the user perspective YOLO proved to be very easy to use and setup. Indeed because of the time restriction when using the Google Colab free tier we decided to install locally all necessary drivers (NVIDIA, CUDA) and compile locally the [Darknet architecture](https://github.com/AlexeyAB/darknet). This has been done on a Linux computer running Ubuntu 20.04, with 32GB of RAM, NVIDIA GeForce GTX1060 graphic card with 6GB memory and an Intel i7 processor.

To evaluate the model we relied on two metrics: the **mean average precision** (mAP) and the **intersection over union** (IoU). The **average precision** (AP) is a way to get a fair idea of the model performance. It consists of computing the maximum precision we can get at different threshold of recall. Then we calculate the mean of these maximum precision. If we had several classes we need to get the AP for each class and then compute the mean again. This is why this metric is named **mean average precision**. In our case because we only have on class AP and mAP are equals. Object detection brings an additional complexity: what if the model detects the correct class but at the wrong location meaning that the bounding box is completely off. Surely this prediction should not be counted as positive. That is where the IoU comes handy and allows to determine whether the bounding box is located at the right location. Usually a threshold of 0.5 is set and results above are considered as good prediction. As such the corresponding mAP is noted as **mAP@0.5**. The principle of the IoU is depicted in Figure 3.